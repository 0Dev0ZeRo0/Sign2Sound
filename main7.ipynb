{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c2a9dcc-f05e-4df7-b59b-03baf8838c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded labels: ['One' 'Great' 'Young' 'Open' 'Strong' 'Water']\n",
      "Using GPU: NVIDIA GeForce GTX 1650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Local\\Temp\\ipykernel_37196\\2546285222.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH))\n"
     ]
    }
   ],
   "source": [
    "## Cell 1: Imports and Setup\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "from corpus_engine import CorpusEngine\n",
    "from audiofication import generate_audio\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.7)\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.7)\n",
    "\n",
    "# Paths and parameters\n",
    "MODEL_PATH = 'models/sign_lstm.pth'\n",
    "LABELS_PATH = 'processed_data/labels.npy'\n",
    "FRAMES_PER_VIDEO = 30\n",
    "EXPECTED_LANDMARKS = 225  # 21*3*2 (hands) + 33*3 (pose)\n",
    "MOVEMENT_FRAMES = 10  # Number of frames to capture movement (1-2 seconds at 5-10 fps)\n",
    "\n",
    "# Load labels\n",
    "try:\n",
    "    labels = np.load(LABELS_PATH)\n",
    "    print(f\"Loaded labels: {labels}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {LABELS_PATH} not found. Run preprocess_data.py.\")\n",
    "    raise\n",
    "\n",
    "# Initialize corpus engine\n",
    "from corpus_engine import SENTENCES\n",
    "corpus_engine = CorpusEngine(SENTENCES)\n",
    "\n",
    "# Verify 10 signs\n",
    "if len(labels) != 6:\n",
    "    print(f\"Error: Expected 9 signs, found {len(labels)}. Update dataset.\")\n",
    "    raise ValueError(\"Incorrect number of signs\")\n",
    "\n",
    "# Define LSTM Model (matching train_model.py)\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size1, hidden_size2, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_size2, hidden_size3, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size3, 64)\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        if len(x.shape) == 3:\n",
    "            x = x[:, -1, :]\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load model\n",
    "input_size = EXPECTED_LANDMARKS\n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 64\n",
    "hidden_size3 = 32\n",
    "output_size = len(labels)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Error: CUDA is not available. Ensure NVIDIA drivers and CUDA are installed.\")\n",
    "    raise RuntimeError(\"CUDA unavailable\")\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "model = LSTMModel(input_size, hidden_size1, hidden_size2, hidden_size3, output_size).to(device)\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to load model. {str(e)}\")\n",
    "        raise\n",
    "else:\n",
    "    print(f\"Error: Model file {MODEL_PATH} not found. Run train_model.py.\")\n",
    "    raise FileNotFoundError(MODEL_PATH)\n",
    "model.eval()\n",
    "\n",
    "# Landmark Extraction with Movement\n",
    "def extract_landmarks_with_movement(frames):\n",
    "    \"\"\"Extract hand and pose landmarks from a sequence of frames to capture movement.\n",
    "    Returns a fixed-size array of landmarks, including motion features.\"\"\"\n",
    "    landmarks_list = []\n",
    "    for frame in frames:\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        hand_results = hands.process(image_rgb)\n",
    "        pose_results = pose.process(image_rgb)\n",
    "        \n",
    "        landmarks = []\n",
    "        if hand_results.multi_hand_landmarks:\n",
    "            for hand_landmarks in hand_results.multi_hand_landmarks[:2]:\n",
    "                for lm in hand_landmarks.landmark:\n",
    "                    landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            landmarks.extend([0] * (21 * 3 * (2 - len(hand_results.multi_hand_landmarks))))\n",
    "        else:\n",
    "            landmarks.extend([0] * (21 * 3 * 2))\n",
    "        \n",
    "        if pose_results.pose_landmarks:\n",
    "            for i, lm in enumerate(pose_results.pose_landmarks.landmark):\n",
    "                if i < 33:\n",
    "                    landmarks.extend([lm.x, lm.y, lm.z])\n",
    "        else:\n",
    "            landmarks.extend([0] * (33 * 3))\n",
    "        \n",
    "        landmarks = np.array(landmarks)\n",
    "        if len(landmarks) != EXPECTED_LANDMARKS:\n",
    "            print(f\"Warning: Landmark array size {len(landmarks)} != {EXPECTED_LANDMARKS}. Padding/truncating.\")\n",
    "            if len(landmarks) < EXPECTED_LANDMARKS:\n",
    "                landmarks = np.pad(landmarks, (0, EXPECTED_LANDMARKS - len(landmarks)), mode='constant')\n",
    "            else:\n",
    "                landmarks = landmarks[:EXPECTED_LANDMARKS]\n",
    "        landmarks_list.append(landmarks)\n",
    "    \n",
    "    # Pad or truncate to FRAMES_PER_VIDEO\n",
    "    landmarks_array = np.array(landmarks_list)\n",
    "    if len(landmarks_list) < FRAMES_PER_VIDEO:\n",
    "        padding = np.zeros((FRAMES_PER_VIDEO - len(landmarks_list), EXPECTED_LANDMARKS))\n",
    "        landmarks_array = np.vstack((landmarks_array, padding))\n",
    "    elif len(landmarks_list) > FRAMES_PER_VIDEO:\n",
    "        landmarks_array = landmarks_array[:FRAMES_PER_VIDEO]\n",
    "    \n",
    "    return landmarks_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4ad42c-68a9-4757-87cb-da29e854b5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax scores for all classes: {'One': 0.5513083, 'Great': 0.15210553, 'Young': 0.073507376, 'Open': 0.15018074, 'Strong': 0.04798989, 'Water': 0.02490818}\n",
      "Detected sign: One (Confidence: 0.55)\n",
      "Multiple sentences with same first word, please show second sign\n",
      "Resetting: Clearing sentence and resuming sign collection\n",
      "Reset triggered by 'r' key!\n",
      "Softmax scores for all classes: {'One': 0.12568732, 'Great': 0.025080798, 'Young': 0.42948085, 'Open': 0.40009362, 'Strong': 0.012019877, 'Water': 0.007637632}\n",
      "Detected sign: Young (Confidence: 0.43)\n",
      "Unique sentence found: Young child learns fast (Triggering audio)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "    Error 305 for command:\n",
      "        open \"C:\\Users\\dhanu\\AppData\\Local\\Temp\\tmpq0vd40am.mp3\"\n",
      "    Cannot specify extra characters after a string enclosed in quotation marks.\n",
      "\n",
      "    Error 305 for command:\n",
      "        close \"C:\\Users\\dhanu\\AppData\\Local\\Temp\\tmpq0vd40am.mp3\"\n",
      "    Cannot specify extra characters after a string enclosed in quotation marks.\n",
      "Failed to close the file: \"C:\\Users\\dhanu\\AppData\\Local\\Temp\\tmpq0vd40am.mp3\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file saved to C:\\Users\\dhanu\\AppData\\Local\\Temp\\tmpq0vd40am.mp3\n",
      "Error generating audio: \n",
      "    Error 305 for command:\n",
      "        open \"C:\\Users\\dhanu\\AppData\\Local\\Temp\\tmpq0vd40am.mp3\"\n",
      "    Cannot specify extra characters after a string enclosed in quotation marks.\n",
      "Warning: Audio playback failed. Check audiofication.py logs.\n",
      "Resetting: Clearing sentence and resuming sign collection\n",
      "Reset triggered by 'r' key (paused state)!\n",
      "Softmax scores for all classes: {'One': 0.13362835, 'Great': 0.02719393, 'Young': 0.42146125, 'Open': 0.39710543, 'Strong': 0.012615479, 'Water': 0.00799553}\n",
      "Detected sign: Young (Confidence: 0.42)\n",
      "Unique sentence found: Young child learns fast (Triggering audio)\n",
      "Audio file saved to C:\\Users\\dhanu\\AppData\\Local\\Temp\\tmpt8vs1r6a.mp3\n",
      "Audio played successfully for: Young child learns fast\n",
      "Resetting: Clearing sentence and resuming sign collection\n",
      "Reset triggered by 'r' key (paused state)!\n",
      "Softmax scores for all classes: {'One': 0.3139079, 'Great': 0.6399801, 'Young': 0.007873458, 'Open': 0.00825719, 'Strong': 0.0143539645, 'Water': 0.015627524}\n",
      "Detected sign: Great (Confidence: 0.64)\n",
      "Unique sentence found: Great job well done (Triggering audio)\n",
      "Audio file saved to C:\\Users\\dhanu\\AppData\\Local\\Temp\\tmpzncpfd0r.mp3\n",
      "Audio played successfully for: Great job well done\n",
      "Resetting: Clearing sentence and resuming sign collection\n",
      "Reset triggered by 'r' key (paused state)!\n",
      "Softmax scores for all classes: {'One': 0.5958487, 'Great': 0.24127586, 'Young': 0.038502637, 'Open': 0.07039052, 'Strong': 0.03440911, 'Water': 0.019573146}\n",
      "Detected sign: One (Confidence: 0.60)\n",
      "Multiple sentences with same first word, please show second sign\n",
      "Resetting: Clearing sentence and resuming sign collection\n",
      "Reset triggered by 'r' key!\n",
      "Softmax scores for all classes: {'One': 0.58832175, 'Great': 0.19070105, 'Young': 0.045876406, 'Open': 0.096974924, 'Strong': 0.050967105, 'Water': 0.027158791}\n",
      "Detected sign: One (Confidence: 0.59)\n",
      "Multiple sentences with same first word, please show second sign\n",
      "Resetting: Clearing sentence and resuming sign collection\n",
      "Reset triggered by 'r' key!\n",
      "Softmax scores for all classes: {'One': 0.59265465, 'Great': 0.24608843, 'Young': 0.037881743, 'Open': 0.06865822, 'Strong': 0.034776334, 'Water': 0.01994074}\n",
      "Detected sign: One (Confidence: 0.59)\n",
      "Multiple sentences with same first word, please show second sign\n",
      "Resetting: Clearing sentence and resuming sign collection\n",
      "Reset triggered by 'r' key!\n",
      "Softmax scores for all classes: {'One': 0.32194862, 'Great': 0.6279222, 'Young': 0.00870781, 'Open': 0.009156389, 'Strong': 0.015571561, 'Water': 0.016693447}\n",
      "Detected sign: Great (Confidence: 0.63)\n",
      "Unique sentence found: Great job well done (Triggering audio)\n",
      "Audio file saved to C:\\Users\\dhanu\\AppData\\Local\\Temp\\tmp8be7vdll.mp3\n",
      "Audio played successfully for: Great job well done\n",
      "Resetting: Clearing sentence and resuming sign collection\n",
      "Reset triggered by 'r' key (paused state)!\n",
      "Softmax scores for all classes: {'One': 0.59016, 'Great': 0.18932277, 'Young': 0.046699814, 'Open': 0.09840778, 'Strong': 0.049297977, 'Water': 0.026111629}\n",
      "Detected sign: One (Confidence: 0.59)\n",
      "Multiple sentences with same first word, please show second sign\n",
      "Resetting: Clearing sentence and resuming sign collection\n",
      "Reset triggered by 'r' key!\n",
      "Softmax scores for all classes: {'One': 0.59443706, 'Great': 0.19658029, 'Young': 0.0504715, 'Open': 0.09835224, 'Strong': 0.039203018, 'Water': 0.020955896}\n",
      "Detected sign: One (Confidence: 0.59)\n",
      "Multiple sentences with same first word, please show second sign\n",
      "Resetting: Clearing sentence and resuming sign collection\n",
      "Reset triggered by 'r' key!\n",
      "Softmax scores for all classes: {'One': 0.31491426, 'Great': 0.6383065, 'Young': 0.008078124, 'Open': 0.008467016, 'Strong': 0.014516867, 'Water': 0.015717302}\n",
      "Detected sign: Great (Confidence: 0.64)\n",
      "Unique sentence found: Great job well done (Triggering audio)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "    Error 259 for command:\n",
      "        play \"C:\\Users\\dhanu\\AppData\\Local\\Temp\\tmp230fh34z.mp3\" wait\n",
      "    The driver cannot recognize the specified command parameter.\n",
      "\n",
      "    Error 305 for command:\n",
      "        close \"C:\\Users\\dhanu\\AppData\\Local\\Temp\\tmp230fh34z.mp3\"\n",
      "    Cannot specify extra characters after a string enclosed in quotation marks.\n",
      "Failed to close the file: \"C:\\Users\\dhanu\\AppData\\Local\\Temp\\tmp230fh34z.mp3\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file saved to C:\\Users\\dhanu\\AppData\\Local\\Temp\\tmp230fh34z.mp3\n",
      "Error generating audio: \n",
      "    Error 259 for command:\n",
      "        play \"C:\\Users\\dhanu\\AppData\\Local\\Temp\\tmp230fh34z.mp3\" wait\n",
      "    The driver cannot recognize the specified command parameter.\n",
      "Warning: Audio playback failed. Check audiofication.py logs.\n",
      "Resetting: Clearing sentence and resuming sign collection\n",
      "Reset triggered by 'r' key (paused state)!\n",
      "Softmax scores for all classes: {'One': 0.5987637, 'Great': 0.22224902, 'Young': 0.042913027, 'Open': 0.08030811, 'Strong': 0.03590873, 'Water': 0.019857407}\n",
      "Detected sign: One (Confidence: 0.60)\n",
      "Multiple sentences with same first word, please show second sign\n",
      "Resources released successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def reset_state():\n",
    "    global sentence, predicted_words, sequence, movement_buffer, current_word, is_paused, sign_start_time, awaiting_second_scan\n",
    "    print(\"Resetting: Clearing sentence and resuming sign collection\")\n",
    "    sentence = None\n",
    "    predicted_words = []\n",
    "    sequence.clear()\n",
    "    movement_buffer.clear()\n",
    "    current_word = None\n",
    "    is_paused = False\n",
    "    sign_start_time = None\n",
    "    awaiting_second_scan = False\n",
    "\n",
    "def real_time_recognition():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    \n",
    "    # Initialize state variables\n",
    "    global sentence, predicted_words, sequence, movement_buffer, current_word, is_paused, sign_start_time, awaiting_second_scan\n",
    "    movement_buffer = deque(maxlen=MOVEMENT_FRAMES)\n",
    "    sequence = deque(maxlen=FRAMES_PER_VIDEO)\n",
    "    predicted_words = []\n",
    "    sentence = None\n",
    "    current_word = None\n",
    "    is_paused = False\n",
    "    sign_start_time = None\n",
    "    awaiting_second_scan = False\n",
    "    \n",
    "    window_name = 'Sign Language Recognition'\n",
    "    cv2.namedWindow(window_name)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Failed to capture frame\")\n",
    "                break\n",
    "            \n",
    "            # Check for keys\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('r'):  # Reset key changed from 's' to 'r'\n",
    "                # Reset when paused or after a sentence is framed\n",
    "                if is_paused:\n",
    "                    reset_state()\n",
    "                    print(\"Reset triggered by 'r' key (paused state)!\")\n",
    "                # Reset at any other point\n",
    "                elif len(predicted_words) > 0:\n",
    "                    reset_state()\n",
    "                    print(\"Reset triggered by 'r' key!\")\n",
    "            elif key == ord('s'):  # Recapture first sign when asking for second sign\n",
    "                if len(predicted_words) == 1:\n",
    "                    first_word = predicted_words[0].lower()\n",
    "                    matching_sentences = [s for s in corpus_engine.sentences if s.lower().startswith(first_word)]\n",
    "                    if len(matching_sentences) > 1:\n",
    "                        predicted_words.clear()\n",
    "                        current_word = None\n",
    "                        movement_buffer.clear()\n",
    "                        sign_start_time = None\n",
    "                        print(\"Recapturing first sign triggered by 's' key!\")\n",
    "            elif key == ord('a') and awaiting_second_scan and len(predicted_words) == 1:\n",
    "                print(\"Rescanning for 2nd word...\")\n",
    "                awaiting_second_scan = False\n",
    "                movement_buffer.clear()\n",
    "                start_time = time.time()\n",
    "                while len(movement_buffer) < MOVEMENT_FRAMES and (time.time() - start_time) < 2:\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    movement_buffer.append(frame)\n",
    "                    cv2.waitKey(100)\n",
    "                if len(movement_buffer) == MOVEMENT_FRAMES:\n",
    "                    landmarks_array = extract_landmarks_with_movement(movement_buffer)\n",
    "                    sequence_array = np.array([landmarks_array])\n",
    "                    sequence_tensor = torch.tensor(sequence_array, dtype=torch.float32).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        output = model(sequence_tensor)\n",
    "                        pred_idx = torch.argmax(output, dim=1).item()\n",
    "                        confidence = torch.softmax(output, dim=1)[0, pred_idx].item()\n",
    "                        # Debug: Print softmax scores for all classes\n",
    "                        softmax_scores = torch.softmax(output, dim=1)[0].cpu().numpy()\n",
    "                        print(f\"Softmax scores for all classes: {dict(zip(labels, softmax_scores))}\")\n",
    "                        if confidence > 0.4:\n",
    "                            new_word = labels[pred_idx]\n",
    "                            predicted_words[1:] = [new_word]\n",
    "                            print(f\"Rescanned 2nd sign: {new_word} (Confidence: {confidence:.2f})\")\n",
    "                            sentence = corpus_engine.frame_sentence_from_sequence(predicted_words)\n",
    "                            if sentence:\n",
    "                                print(f\"Framed sentence: {sentence} (Triggering audio)\")\n",
    "                                success = generate_audio(sentence)\n",
    "                                if not success:\n",
    "                                    print(\"Warning: Audio playback failed. Check audiofication.py logs.\")\n",
    "                                is_paused = True\n",
    "                            else:\n",
    "                                first_word = predicted_words[0].lower()\n",
    "                                second_word = predicted_words[1].lower()\n",
    "                                matching_sentences = [s for s in corpus_engine.sentences if s.lower().startswith(f\"{first_word} {second_word}\")]\n",
    "                                if len(matching_sentences) > 1:\n",
    "                                    print(\"Multiple sentences with same first and second words, please show third sign\")\n",
    "                                    cv2.putText(frame, \"Multiple sentences, show 3rd sign\", \n",
    "                                                (10, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                                    cv2.imshow(window_name, frame)\n",
    "                                    cv2.waitKey(1)\n",
    "                                    \n",
    "                                    movement_buffer.clear()\n",
    "                                    start_time = time.time()\n",
    "                                    while len(movement_buffer) < MOVEMENT_FRAMES and (time.time() - start_time) < 2:\n",
    "                                        ret, frame = cap.read()\n",
    "                                        if not ret:\n",
    "                                            break\n",
    "                                        movement_buffer.append(frame)\n",
    "                                        cv2.waitKey(100)\n",
    "                                    \n",
    "                                    if len(movement_buffer) == MOVEMENT_FRAMES:\n",
    "                                        landmarks_array = extract_landmarks_with_movement(movement_buffer)\n",
    "                                        sequence_array = np.array([landmarks_array])\n",
    "                                        sequence_tensor = torch.tensor(sequence_array, dtype=torch.float32).to(device)\n",
    "                                        with torch.no_grad():\n",
    "                                            output = model(sequence_tensor)\n",
    "                                            pred_idx = torch.argmax(output, dim=1).item()\n",
    "                                            confidence = torch.softmax(output, dim=1)[0, pred_idx].item()\n",
    "                                            softmax_scores = torch.softmax(output, dim=1)[0].cpu().numpy()\n",
    "                                            print(f\"Softmax scores for all classes: {dict(zip(labels, softmax_scores))}\")\n",
    "                                            if confidence > 0.4:\n",
    "                                                new_word = labels[pred_idx]\n",
    "                                                predicted_words.append(new_word)\n",
    "                                                print(f\"Detected third sign: {new_word} (Confidence: {confidence:.2f})\")\n",
    "                                                sentence = corpus_engine.frame_sentence_from_sequence(predicted_words)\n",
    "                                                if sentence:\n",
    "                                                    print(f\"Framed sentence: {sentence} (Triggering audio)\")\n",
    "                                                    success = generate_audio(sentence)\n",
    "                                                    if not success:\n",
    "                                                        print(\"Warning: Audio playback failed. Check audiofication.py logs.\")\n",
    "                                                    is_paused = True\n",
    "                                                else:\n",
    "                                                    print(\"No unique sentence found after three words, resetting to first word\")\n",
    "                                                    predicted_words = predicted_words[:1]\n",
    "                                                    current_word = predicted_words[-1] if predicted_words else None\n",
    "                                                    awaiting_second_scan = True\n",
    "                                        movement_buffer.clear()\n",
    "                                    else:\n",
    "                                        break\n",
    "                                else:\n",
    "                                    print(\"Rescanned 2nd word still ambiguous, awaiting further input\")\n",
    "                                    awaiting_second_scan = True\n",
    "                        movement_buffer.clear()\n",
    "                continue\n",
    "            \n",
    "            if is_paused:\n",
    "                sentence_text = f\"Sentence: {sentence if sentence else 'Detecting...'}\"\n",
    "                word_text = f\"Current Word: {current_word if current_word else 'None'}\"\n",
    "                cv2.putText(frame, sentence_text, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                cv2.putText(frame, word_text, (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                cv2.putText(frame, \"Press 'r' to reset\", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                cv2.imshow(window_name, frame)\n",
    "                continue\n",
    "            \n",
    "            movement_buffer.append(frame)\n",
    "            \n",
    "            if len(movement_buffer) == 1 and sign_start_time is None:\n",
    "                sign_start_time = time.time()\n",
    "            \n",
    "            if len(movement_buffer) == MOVEMENT_FRAMES and sign_start_time and (time.time() - sign_start_time) >= 3:\n",
    "                try:\n",
    "                    landmarks_array = extract_landmarks_with_movement(movement_buffer)\n",
    "                    if landmarks_array.shape != (FRAMES_PER_VIDEO, EXPECTED_LANDMARKS):\n",
    "                        print(f\"Warning: Invalid landmarks array shape {landmarks_array.shape}. Skipping.\")\n",
    "                        movement_buffer.clear()\n",
    "                        current_word = None\n",
    "                        sign_start_time = None\n",
    "                        continue\n",
    "                    \n",
    "                    # Debug: Check if landmarks have valid values\n",
    "                    if np.isnan(landmarks_array).any() or np.isinf(landmarks_array).any():\n",
    "                        print(\"Warning: Landmarks contain NaN or Inf values. Skipping.\")\n",
    "                        movement_buffer.clear()\n",
    "                        current_word = None\n",
    "                        sign_start_time = None\n",
    "                        continue\n",
    "                    \n",
    "                    sequence_array = np.array([landmarks_array])\n",
    "                    sequence_tensor = torch.tensor(sequence_array, dtype=torch.float32).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        output = model(sequence_tensor)\n",
    "                        pred_idx = torch.argmax(output, dim=1).item()\n",
    "                        confidence = torch.softmax(output, dim=1)[0, pred_idx].item()\n",
    "                        # Debug: Print softmax scores for all classes\n",
    "                        softmax_scores = torch.softmax(output, dim=1)[0].cpu().numpy()\n",
    "                        print(f\"Softmax scores for all classes: {dict(zip(labels, softmax_scores))}\")\n",
    "                        \n",
    "                        if confidence > 0.4:\n",
    "                            new_word = labels[pred_idx]\n",
    "                            current_word = new_word\n",
    "                            movement_buffer.clear()\n",
    "                            sign_start_time = None\n",
    "                            print(f\"Detected sign: {new_word} (Confidence: {confidence:.2f})\")\n",
    "                            \n",
    "                            predicted_words.append(new_word)\n",
    "                            \n",
    "                            # Check for matching sentences based on the current predicted words\n",
    "                            if len(predicted_words) == 1:\n",
    "                                first_word = predicted_words[0].lower()\n",
    "                                matching_sentences = [s for s in corpus_engine.sentences if s.lower().startswith(first_word)]\n",
    "                                if len(matching_sentences) > 1:\n",
    "                                    print(\"Multiple sentences with same first word, please show second sign\")\n",
    "                                    cv2.putText(frame, \"Multiple sentences, show 2nd sign\", \n",
    "                                                (10, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                                    cv2.putText(frame, \"First sign incorrect? Press 's' to recapture\", \n",
    "                                                (10, 190), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                                    cv2.putText(frame, \"Press 'r' to reset\", \n",
    "                                                (10, 220), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                                    cv2.imshow(window_name, frame)\n",
    "                                    cv2.waitKey(1)\n",
    "                                    continue\n",
    "                                elif len(matching_sentences) == 1:\n",
    "                                    sentence = matching_sentences[0]\n",
    "                                    print(f\"Unique sentence found: {sentence} (Triggering audio)\")\n",
    "                                    success = generate_audio(sentence)\n",
    "                                    if not success:\n",
    "                                        print(\"Warning: Audio playback failed. Check audiofication.py logs.\")\n",
    "                                    is_paused = True\n",
    "                                    continue\n",
    "                            elif len(predicted_words) == 2:\n",
    "                                first_word = predicted_words[0].lower()\n",
    "                                second_word = predicted_words[1].lower()\n",
    "                                matching_sentences = [s for s in corpus_engine.sentences if s.lower().startswith(f\"{first_word} {second_word}\")]\n",
    "                                if len(matching_sentences) > 1:\n",
    "                                    print(\"Multiple sentences with same first and second words, please show third sign\")\n",
    "                                    cv2.putText(frame, \"Multiple sentences, show 3rd sign\", \n",
    "                                                (10, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                                    cv2.imshow(window_name, frame)\n",
    "                                    cv2.waitKey(1)\n",
    "                                    continue\n",
    "                                elif len(matching_sentences) == 1:\n",
    "                                    sentence = matching_sentences[0]\n",
    "                                    print(f\"Unique sentence found: {sentence} (Triggering audio)\")\n",
    "                                    success = generate_audio(sentence)\n",
    "                                    if not success:\n",
    "                                        print(\"Warning: Audio playback failed. Check audiofication.py logs.\")\n",
    "                                    is_paused = True\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    print(\"2nd word may be incorrect, press 'a' to scan again for 2nd word\")\n",
    "                                    cv2.putText(frame, \"2nd word incorrect? Press 'a' to rescan\", \n",
    "                                                (10, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                                    awaiting_second_scan = True\n",
    "                                    cv2.imshow(window_name, frame)\n",
    "                                    cv2.waitKey(1)\n",
    "                                    continue\n",
    "                            elif len(predicted_words) == 3:\n",
    "                                first_word = predicted_words[0].lower()\n",
    "                                second_word = predicted_words[1].lower()\n",
    "                                third_word = predicted_words[2].lower()\n",
    "                                matching_sentences = [s for s in corpus_engine.sentences if s.lower().startswith(f\"{first_word} {second_word} {third_word}\")]\n",
    "                                if len(matching_sentences) == 1:\n",
    "                                    sentence = matching_sentences[0]\n",
    "                                    print(f\"Unique sentence found: {sentence} (Triggering audio)\")\n",
    "                                    success = generate_audio(sentence)\n",
    "                                    if not success:\n",
    "                                        print(\"Warning: Audio playback failed. Check audiofication.py logs.\")\n",
    "                                    is_paused = True\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    print(\"No unique sentence found after three words, resetting to first word\")\n",
    "                                    predicted_words = predicted_words[:1]\n",
    "                                    current_word = predicted_words[-1] if predicted_words else None\n",
    "                                    awaiting_second_scan = True\n",
    "                                    cv2.putText(frame, \"2nd word incorrect? Press 'a' to rescan\", \n",
    "                                                (10, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                                    cv2.imshow(window_name, frame)\n",
    "                                    cv2.waitKey(1)\n",
    "                                    continue\n",
    "                        else:\n",
    "                            print(f\"Low confidence: {confidence:.2f}. Skipping prediction.\")\n",
    "                            current_word = None\n",
    "                            movement_buffer.clear()\n",
    "                            sign_start_time = None\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error: Failed to process sequence. {str(e)}\")\n",
    "                    movement_buffer.clear()\n",
    "                    current_word = None\n",
    "                    sign_start_time = None\n",
    "                    continue\n",
    "            \n",
    "            sentence_text = f\"Sentence: {sentence if sentence else 'Detecting...'}\"\n",
    "            word_text = f\"Current Word: {current_word if current_word else 'None'}\"\n",
    "            cv2.putText(frame, sentence_text, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(frame, word_text, (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "            \n",
    "            if awaiting_second_scan:\n",
    "                cv2.putText(frame, \"2nd word incorrect? Press 'a' to rescan\", \n",
    "                            (10, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            cv2.imshow(window_name, frame)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboard interrupt detected. Releasing resources...\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        hands.close()\n",
    "        pose.close()\n",
    "        print(\"Resources released successfully.\")\n",
    "\n",
    "real_time_recognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f2ff7-5ea3-4688-9694-d63699aa40f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpy-torch)",
   "language": "python",
   "name": "gpu-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
